# k-NN Classifier with Cross-Validation and Visualization

This project implements a custom **k-Nearest Neighbors (k-NN)** classifier using Euclidean distance from scratch, visualizes training data and test points, and performs **Stratified K-Fold Cross-Validation** to determine the best `k`.

---

## ðŸ”§ Features

- âœ… Custom k-NN prediction function
- ðŸ“ˆ Data visualization with `matplotlib` and `seaborn`
- ðŸ¤– Optimal `k` selection via cross-validation
- ðŸ” Dynamic `k` generation based on dataset size
- ðŸ§  Smart suggestion of `n_splits` for StratifiedKFold

---

## ðŸ“Œ How It Works
A suitable k is calculated from the number of training samples (âˆšn, made odd).

A test point is classified using the custom knn_predict function.

The dataset is plotted for visual analysis.

Cross-validation is used to find the best k using StratifiedKFold.


## ðŸ“ˆ Visual Output
Scatter plot of training samples categorized by label.

Highlighted test point (X) on the same plot.

Plot of cross-validated accuracy vs. different k values.


## ðŸ§ª Dependencies
Install with pip: pip install numpy pandas seaborn matplotlib scikit-learn



## â–¶ï¸ Run the Script
python main.py



## ðŸ“Š Dataset

- Each data point includes:
  - `Age` (19â€“45)
  - `Salary` (â‚¹15,000â€“30,000)
- Labeled into 3 categories: `A`, `B`, and `C`

> Example:
```python
training_data = [
    [22, 24000], [26, 25500], ...
]
training_labels = ['A', 'A', 'B', ...]



